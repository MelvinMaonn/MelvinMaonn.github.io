<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Road2Vec</title>
      <link href="/2018/05/29/Road2Vec/"/>
      <url>/2018/05/29/Road2Vec/</url>
      <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>最近在做关于路网速度预测的研究，很大一部分的文章都是基于深度学习设计模型和实验的，比如用CNN或者LSTM。尤其是很多文章希望用CNN来抓取路网的拓扑结构。</p><p>如图所示，普通的CNN，只能卷积到0、1、2、3之间的拓扑连接关系，但是难以抓取到其相互间权重关系，因此无法考虑到道路之间影响的时空异质性。比如对于0有1、2、3共三条上游道路，但是哪一条道路对于0的影响更大，CNN的表现并不好。<br><img src="/2018/05/29/Road2Vec/traffic_stream.png" alt="traffic_stream"></p><p>但是有一篇文章提出了一个较为新颖的方法——road2vector，即借用word2vector的方法，来分析道路间的更深层的关系。其可以根据实际大量的车辆行驶轨迹，来确定1、2、3与0的权重，比如从1进入0的车辆最多，那么1与0的权重比2、3都大。<a id="more"></a></p><h1 id="word2vector"><a href="#word2vector" class="headerlink" title="word2vector"></a>word2vector</h1><p>什么是word2vector？<br>借用一个常用的例子，“the dog bark at the mailman”，这句话共有6个单词，如果用one-hot编码来表示每个单词当然是可以的，像下面这样：<br>[1,0,0,0,0,0]<br>[0,1,0,0,0,0]<br>[0,0,1,0,0,0]<br>[0,0,0,1,0,0]<br>[0,0,0,0,1,0]<br>[0,0,0,0,0,1]<br>如此，6个单词需要6维的向量来表示，那么一篇文章如果有1万个单词，岂非要用1万维的向量来表示？no，这不是万维网。</p><p>所以，我们需要word embedding，将高维的稀疏向量降维至低维的稠密向量。而word2vector就是word embedding的一种方式，可以实现单词向量化。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>word2vector有两种常用模型，一种是CBOW，另一种是Skip-Gram。两者的差别在于输入输出的不同，前者是通过上下文来预测一个相关的单词，而后者是通过一个单词来预测其上下文相关的单词。本文重点讨论后一种模型，至于为什么，往下看你就知道了。</p><p>Skip-Gram的模型如下图：<br><img src="/2018/05/29/Road2Vec/Skip_Gram.png" alt="Skip_Gram"><br>其根据一个输入词，预测该词的上下文。同样以“the dog bark at the mailman”为例，当设置window_size(窗口大小，用来限制上下文的范围)为1时，则可以得到：<br>(dog，[the, bark])<br>(bark,  [dog, at])<br>(at, [bark, the])<br>(the, [at,mailman])</p><p>如下图，当输入的样本为(dog, [the,bark])时，dog所对应的三根红线就是训练的权值，即用一个三维的向量表示dog，这就是embedding的过程，由one-hot的六维向量变成了三维向量。所有的样本训练完成之后即得到红框所示的embedding matrix，里面包含了输入与输出的映射关系，我们并不需要输出层，可以直接将输出层砍掉仅保留隐藏层即可，因此这也成为一种“Fake task”。<br><img src="/2018/05/29/Road2Vec/word2vector_model.png" alt="word2vector_model"></p><h1 id="如何解决我们的问题"><a href="#如何解决我们的问题" class="headerlink" title="如何解决我们的问题"></a>如何解决我们的问题</h1><p>我们可以将一条行车轨迹类比为一句话，轨迹中的每一条道路类比为一个单词：<br>南京大街  兴华路  高山路  邮政路  鲁磨路 珞喻路<br>the            dog       bark       at          the    mailman</p><p>使用此种类比方式，我们就可以用word2vector模型抓取到各个道路之间的时空异质性联系，分析出鲁磨路是与珞喻路联系更紧密还是与雄楚大道交互更多。</p><h1 id="存在什么问题"><a href="#存在什么问题" class="headerlink" title="存在什么问题"></a>存在什么问题</h1><p>（1）word2vector的结果是[-1,1]之间的值，而负数值所代表的意义还不明确。<br>（2）车流量是不可能扩散到上游的，就像车不能倒退，水不能倒流，因此对于一条道路，比如鲁磨路，我们更关心的是其下游道路的情况，从而可以更准确的得知该道路的流量向下游那条道路扩散。而window_size是对一个单词的前后取值，既包括上文也包括下文，对我们的结果又一定的影响。</p><h1 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h1><p>问题（1）尚需要去查阅相关资料，问题（2）则可以通过修改模型或者在其它处理阶段进行上下游的限制。</p><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><p>Liu K, Gao S, Qiu P, et al. Road2Vec: Measuring Traffic Interactions in Urban Road System from Massive Travel Routes[J]. ISPRS International Journal of Geo-Information, 2017, 6(11): 321.<br><a href="https://blog.csdn.net/qq_22238533/article/details/78534743" target="_blank" rel="noopener">word2vector原理入门</a><br><a href="https://zhuanlan.zhihu.com/p/27234078" target="_blank" rel="noopener">Skip-Gram模型</a></p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> word embedding </tag>
            
            <tag> traffic prediction </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Amazon爬虫-Spider</title>
      <link href="/2018/05/21/Amazon%E7%88%AC%E8%99%AB-Spider/"/>
      <url>/2018/05/21/Amazon%E7%88%AC%E8%99%AB-Spider/</url>
      <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>目前的网络爬虫工具很多，有按等级收费的，比如八爪鱼；在github上也有很多免费开源的爬虫程序。最近我这边来了一个需求，是需要根据给定的关键词，获取其在亚马逊中搜索结果的排序，并且是10个结果页面。</p><p>亚马逊的搜索结果包含两部分：一个是广告，另一个是实际产品。所以，要获取这两部分的排序，分别称为广告位和自然位。<br><img src="/2018/05/21/Amazon爬虫-Spider/Amazon_key_safe_result.png" alt="Amazon_key_safe_result"></p><p>在最开始的时候，也尝试过使用八爪鱼。但是，八爪鱼对Amazon的支持并不好。即使中间付费定制了规则，使得其能够正常爬取，但是因为八爪鱼的内置浏览器不是类似于谷歌这一类主流浏览器，而Amzon返回的结果跟浏览器有关，所以从八爪鱼上爬取到的结果与主流浏览器的结果并不一样，所以不具有实际意义。</p><p>在github上也找过一些程序，但是关于Amazon的爬虫程序相对较少，有的虽然是关于Amazon的，但是比较重，需要安装各种数据库、框架，安装过程比较复杂，后期维护也比较麻烦。</p><p>最终权衡利弊，只有自己去写一份爬虫程序了。<a id="more"></a></p><h1 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h1><p>首先，虽然最终需要两部分的结果，但是，只要能够把整个结果的整体排序得到，可以单独的再把广告位和自然位区分开。<br>其次，我们只想让爬虫帮我们省去手工查找的时间，所以只要爬虫可以自动化实现爬取就可以了，并不要求爬虫在短时间内爬取结果。<br>最后，因为这个程序最终要给非计算机专业人士使用，所以需要有GUI。</p><h1 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h1><p>1.首先我们需要读取关键词以及将最后的结果写入文件，最好的文件格式就是Excel格式。所以我们需要能够对Excel读取的框架。xlwt只能新建一个excel写入且不支持新的xlsx；xlutils可以改写excel并且支持xlsx，但是其只能在python2中使用。最终权衡，采用了xlwt。<br>2.因为要求爬取结果符合实际搜索结果，并且对爬取时间并不做要求，所以，采用selenium自动化测试框架，录制一段打开网页的脚本，这样即可以获取到实际的搜索结果，并且也减少了因快速爬虫导致的反爬虫机制的限制，毕竟Amazon的反爬虫还是相当强大的。<br>3.对于爬取结果的解析，采用BeautifulSoup框架。<br>4.对于广告的筛选，采用正则表达式。<br>5.对于GUI，采用Tkinter。<br>6.对于打包，采用pyinstaller。</p><h1 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码"></a>关键代码</h1><p>1.读取关键词<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readKeyWord</span><span class="params">(self, filePath)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> keyWords</span><br><span class="line">    excel = xlrd.open_workbook(filePath.get())</span><br><span class="line">    table = excel.sheet_by_name(<span class="string">u'key_word'</span>)</span><br><span class="line">    keyWords = table.col_values(<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p>2.搜索<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">searchKeyWord</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> keyWord <span class="keyword">in</span> keyWords:</span><br><span class="line">        <span class="comment"># 在字典中初始化该关键词相关记录</span></span><br><span class="line">        data = &#123;</span><br><span class="line">            keyWord: &#123;keyWord + <span class="string">'_ad_position'</span>: [], keyWord + <span class="string">'_natural_position'</span>: [], keyWord + <span class="string">'_all_position'</span>: []&#125;&#125;</span><br><span class="line">        self.resultDic.update(data)</span><br><span class="line"></span><br><span class="line">        self.driver.get(<span class="string">"https://www.amazon.co.uk/"</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line">        self.driver.find_element_by_id(<span class="string">"twotabsearchtextbox"</span>).click()</span><br><span class="line">        self.driver.find_element_by_id(<span class="string">"twotabsearchtextbox"</span>).clear()</span><br><span class="line">        self.driver.find_element_by_id(<span class="string">"twotabsearchtextbox"</span>).send_keys(keyWord)</span><br><span class="line">        <span class="comment"># 第一页</span></span><br><span class="line">        self.driver.find_element_by_name(<span class="string">"site-search"</span>).submit()</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line">        self.readPage(keyWord)</span><br><span class="line">        <span class="comment"># 第2-10页</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">11</span>):</span><br><span class="line">            self.driver.find_element_by_id(<span class="string">"pagnNextString"</span>).click()</span><br><span class="line">            time.sleep(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 读取页面信息</span></span><br><span class="line">            self.readPage(keyWord)</span><br><span class="line">    self.driver.close()</span><br></pre></td></tr></table></figure></p><p>3.读取解析页面<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readPage</span><span class="params">(self, keyWord)</span>:</span></span><br><span class="line">    html = self.driver.page_source</span><br><span class="line">    soup = BeautifulSoup(html, features=<span class="string">'lxml'</span>)</span><br><span class="line">    all_result = soup.find_all(<span class="string">'h2'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> result <span class="keyword">in</span> all_result:</span><br><span class="line">        self.resultDic.get(keyWord).get(keyWord + <span class="string">'_all_position'</span>).append(result.get_text())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> re.search(<span class="string">'Sponsored'</span>, result.get_text()) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            self.resultDic.get(keyWord).get(keyWord + <span class="string">'_natural_position'</span>).append(result.get_text())</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        self.resultDic.get(keyWord).get(keyWord + <span class="string">'_ad_position'</span>).append(result.get_text())</span><br></pre></td></tr></table></figure></p><p>4.结果写入excel<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createResultExcel</span><span class="params">(self, outputFilePath)</span>:</span></span><br><span class="line">    workbook = xlwt.Workbook(encoding=<span class="string">'ascii'</span>)</span><br><span class="line"></span><br><span class="line">    keyWordNum = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> keyWord <span class="keyword">in</span> keyWords:</span><br><span class="line">        <span class="comment"># 广告sheet</span></span><br><span class="line">        worksheet = workbook.add_sheet(str(keyWordNum) + <span class="string">'_ad_position'</span>)</span><br><span class="line">        row = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> self.resultDic.get(keyWord).get(keyWord + <span class="string">'_ad_position'</span>):</span><br><span class="line">            worksheet.write(row, <span class="number">0</span>, title)</span><br><span class="line">            row += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自然位sheet</span></span><br><span class="line">        worksheet = workbook.add_sheet(str(keyWordNum) + <span class="string">'_natural_position'</span>)</span><br><span class="line">        row = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> self.resultDic.get(keyWord).get(keyWord + <span class="string">'_natural_position'</span>):</span><br><span class="line">            worksheet.write(row, <span class="number">0</span>, title)</span><br><span class="line">            row += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 整体位sheet</span></span><br><span class="line">        worksheet = workbook.add_sheet(str(keyWordNum) + <span class="string">'_all_position'</span>)</span><br><span class="line">        row = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> self.resultDic.get(keyWord).get(keyWord + <span class="string">'_all_position'</span>):</span><br><span class="line">            worksheet.write(row, <span class="number">0</span>, title)</span><br><span class="line">            row += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        keyWordNum += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    workbook.save(outputFilePath.get()+<span class="string">'/'</span>+self.resultExcelName)</span><br></pre></td></tr></table></figure></p><h1 id="成果"><a href="#成果" class="headerlink" title="成果"></a>成果</h1><p>最终的成型软件打开是如下所示（因为功能较单一，所以在GUI上没花太大功夫，只要易于使用即可）<br><img src="/2018/05/21/Amazon爬虫-Spider/Spider_GUI.png" alt="Spider_GUI"></p><p>爬取的结果在excel文件里，下图是广告位排名：<br><img src="/2018/05/21/Amazon爬虫-Spider/keysafe_sponsored_position.png" alt="keysafe_sponsored_position"><br>下图是自然位排名：<br><img src="/2018/05/21/Amazon爬虫-Spider/key_safe_natural_position.png" alt="key_safe_natural_position"><br>下图是全部结果的整体位排名：<br><img src="/2018/05/21/Amazon爬虫-Spider/key_safe_all_position.png" alt="key_safe_all_position"></p><p>由于篇幅限制，本文只说明了一些关键代码，如果想要了解全部代码，请移步至<a href="https://github.com/MelvinMaonn/spider" target="_blank" rel="noopener">Spider</a></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python爬虫 </tag>
            
            <tag> Amazon </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>数据库索引</title>
      <link href="/2018/04/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/"/>
      <url>/2018/04/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/</url>
      <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本人在做实验的过程中，对于一个有千万级数据量的表，需要对其有查询操作，而其中一个查询列是只有0、1的二值列，传统的检验都建议这种值单一的列不要建索引，但是经过实际测试，在该列建立索引后，查询速度要提升5倍以上，原来处理一个表需要接近7个小时，而建立索引之后只需要1个小时左右，因此，希望通过这边文章，探究数据库索引的内在机制。<a id="more"></a></p><h1 id="为什么需要索引"><a href="#为什么需要索引" class="headerlink" title="为什么需要索引"></a>为什么需要索引</h1><p>现代数据库中，数据存储的基本单位是元组，而元组的更高一级是表，其数据结构称为阵列，也类似于编程语言中的二维数组。每一个表的元组共享相同的组成结构，其关系如下图：<br><img src="/2018/04/16/数据库索引/table.png" alt="表"></p><p>如上所示，当对普通数据表进行查询时，会遍历该表，所以，该表的数据量有多大，所遍历的数据量就有多大，两者是线性关系，所以这种查询的时间复杂度为O(n)。</p><h1 id="索引为什么可以加快查询"><a href="#索引为什么可以加快查询" class="headerlink" title="索引为什么可以加快查询"></a>索引为什么可以加快查询</h1><h2 id="二叉搜索树"><a href="#二叉搜索树" class="headerlink" title="二叉搜索树"></a>二叉搜索树</h2><p>对于二叉搜索树我们都很熟悉了，像下面这张图：<br><img src="/2018/04/16/数据库索引/binary_search_tree.png" alt="二叉搜索树"></p><p>二叉搜索树其实本质就是二分搜索，每次将搜索范围缩小一半，所以整体的时间复杂度为O(logn)。</p><h2 id="B-树"><a href="#B-树" class="headerlink" title="B+树"></a>B+树</h2><p>二叉搜索树在搜索单条数据的时候很有效，但是对于某一个范围内的数据时，就要搜索整棵树了，所以时间复杂度又变成了O(n)。</p><p>因此，需要采用B+树这种数据结构，如下图所示：<br><img src="/2018/04/16/数据库索引/B+_tree.png" alt="二叉搜索书"><br>B+树的数据都存放在其叶子节点上，各个叶子节点间有序且相连，所以当搜索某一范围的值时，只需要找到其左边界的值，即可沿着其叶子节点逐个向后遍历，直到找到右边界。整个过程只有搜索左边界的时间复杂度是O(logn)，而遍历后续值的时间复杂度是常量级，因此整个时间复杂度是O(logn)。</p><h1 id="索引的缺点"><a href="#索引的缺点" class="headerlink" title="索引的缺点"></a>索引的缺点</h1><ol><li>索引本身也是一种数据结构，也需要占用一定的存储空间，尤其是聚簇索引需要占据更大的空间</li><li>索引的建立与维护也需要消耗时间，尤其是对数据进行修改时，索引需要动态的改变，所以索引虽然可以加快查询的速度，但是也会减慢增、删、改的速度</li></ol><h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>所以，对于是否要使用索引，首先要理解索引的实现原理，并且结合自己的实际项目需求，不可对于网上的说法亦步亦趋。</p>]]></content>
      
      <categories>
          
          <category> Database </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
            <tag> 索引 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ST-DBSCAN</title>
      <link href="/2018/04/06/ST-DBSCAN/"/>
      <url>/2018/04/06/ST-DBSCAN/</url>
      <content type="html"><![CDATA[<h1 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h1><p>目前人手一部的智能手机都有GPS定位功能，这就使得获取人们的位置信息越来越容易。传统的研究，往往认为位置确定角色，即通过空间位置的变化确定一个人的信息。而实际上，同一地点，时间不同的人，也会有巨大的差别。所以，我们需要从<strong>时间</strong>和<strong>空间</strong>两个维度来确定个体的角色，从而确定不同的用户组别并为个体归类。</p><p>用户的组别信息，对于社会的各个行业乃至政府的决策都是至关重要的，有了这些信息，服务提供者可以锁定服务需求者，商品出售者可以直接定位顾客，政府据此进行城市规划、制定决策等。由此可见，对用户的聚类分析是极具研究价值和市场潜力的。</p><p>而多维活动序列元素的组成的不同，如&lt;地点&gt;、&lt;地点、持续时间&gt;、&lt;活动类型、地点、持续时间、开始时间&gt;三种不同的元素序列，对于用户聚类分析的结果也是有不同的影响的。</p><p>因此，本项目主要目的是根据行人的一系列轨迹（在此称为定位点，即Stop点），来找到用户的兴趣点（POI，Point of Interests），为进一步刻画用户做准备。<a id="more"></a></p><h1 id="常见聚类算法"><a href="#常见聚类算法" class="headerlink" title="常见聚类算法"></a>常见聚类算法</h1><p>聚类就是按照某个特定标准(如距离准则)把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同数据尽量分离。</p><p>目前，有大量的聚类算法。而对于具体应用，聚类算法的选择取决于数据的类型、聚类的目的。如果聚类分析被用作描述或探查的工具，可以对同样的数据尝试多种算法，以发现数据可能揭示的结果。<br>　<br>主要的聚类算法可以划分为如下几类：划分方法、层次方法、基于密度的方法、基于网格的方法以及基于模型的方法。</p><h1 id="双指针遍历"><a href="#双指针遍历" class="headerlink" title="双指针遍历"></a>双指针遍历</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>通常情况下，在某一地点停留一定时间（如15min），即可认为用户在该地点的活动有一定的意义。当定位点（Stop）采集频率为5min一次时，则连续4个Stop点的位置的距离在一定范围内（如50m）即可确定一个活动，可以将该位置抽象为一个POI（Point of Interests），具有经纬度、开始时间、持续时间等属性。</p><p>如下图所示，是定位设备按照每5min采集一次经纬度，所得到的行人一天的轨迹信息。<br><img src="/2018/04/06/ST-DBSCAN/stop_15min.png" alt="stop_15min"></p><h2 id="算法详述"><a href="#算法详述" class="headerlink" title="算法详述"></a>算法详述</h2><p>此聚类算法，适用于采集定位时间间隔较长，少数Stop点即可确定一个POI的情况，其时空复杂度较小。<br>如下图，以链表的形式表示连续的Stop点，以红色箭头（RA）和绿色箭头（GA）由前至后遍历链表。<br><img src="/2018/04/06/ST-DBSCAN/double_pointer_cross_list.png" alt="double_pointer_cross_list"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">初始状态下，RA指向1，GA指向2。</span><br><span class="line">⑴若RA与GA所指Stop点的距离小于50m，</span><br><span class="line">则GA向后移动1个步长；</span><br><span class="line">否则，</span><br><span class="line">若RA此时代表一个POI并且容忍值为true，</span><br><span class="line">则GA均向后移动一个步长，容忍值设为false；</span><br><span class="line">否则，</span><br><span class="line">    GA和RA均向后移动1个步长</span><br><span class="line">⑵当RA与GA的间隔为3个步长（即ID之差大于等于3），则可以认为RA与GA之间的点在一定的范围 内，此时可以将RA所指的点近似抽象为一个POI，容忍值设为true。</span><br></pre></td></tr></table></figure></p><p>最终的聚类结果如下：<br><img src="/2018/04/06/ST-DBSCAN/stop_15min_result.png" alt="stop_15min_result"></p><h1 id="ST-DBSCAN"><a href="#ST-DBSCAN" class="headerlink" title="ST-DBSCAN"></a>ST-DBSCAN</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>由上图可以看出，三个聚类的中心，其实也应该是一个POI，但是由于采样频率太低，导致没有形成聚类。但是，双指针遍历链表的算法是有一定的局限性的，即它的聚类下限必须很低才可以形成有效聚类。当采样频率提高之后，双指针遍历链表的算法就不再适用了。</p><p>因此，我们需要更高级的聚类算法，即ST-DBSCAN，才能满足我们的需求。</p><h2 id="算法详述-1"><a href="#算法详述-1" class="headerlink" title="算法详述"></a>算法详述</h2><p><strong>DBSCAN</strong>，英文全写为Density-based spatial clustering of applications with noise ，是在 1996 年由Martin Ester, Hans-Peter Kriegel, Jörg Sander 及 Xiaowei Xu 提出的聚类分析算法， 这个算法是以密度为本的：给定某空间里的一个点集合，这算法能把附近的点分成一组（有很多相邻点的点），并标记出位于低密度区域的局外点（最接近它的点也十分远），DBSCAN 是其中一个最常用的聚类分析算法，也是其中一个科学文章中最常引用的。</p><p>而ST—DBSCAN则是从Spatial（时间，或者其它非空间属性，如温度）和Temporal（空间）两个维度来对数据进行聚类，其伪代码如下：<br><img src="/2018/04/06/ST-DBSCAN/pesudo_code_st_dbscan.png" alt="pesudo_code_st_dbscan"></p><p>此方法的时空复杂度较高，适用于采集定位时间间隔较短（如30s），需要大量Stop点（如20个）才能确定一个POI的情况。</p><p>而本系统在参照经典的ST—DBSCAN算法前提下，根据数据在时间维度有序的特点，对该方法做了改进（以10min确定一个POI，采集定位时间间隔为30s，此时Eps2为20）：<br>1.因为Stop为连续有序的，并且两个Stop点的时间间隔是确定的，所以可以ID之差代表时间之差，减少了时间转换所带来的复杂度。<br>2.对于Retrieve_Neighbors(CurrentObj,Eps1,Eps2)方法，当CurrentObj为78时（如下图所示），由于当57之后的Stop点与78的时间之差都大于10min，所以57及之前的Stop点在Temporal维度上已经不可能与78形成一个聚类，所以无需再遍历57及之前的Stop；同理，99及之后的Stop点也无需遍历。这个改变，只需要处理CurrentObj的前后Eps2个Stop点即可（58—98），避免了每处理一个CurrentObj时就遍历所有的Stop点，极大地减少了时间复杂度。<br>3.但是，又因为手机设备经常会因为本系统耗电较多而终止本系统，所以存在着两个相邻点实际时间间隔大于30s，比如在系统在获取58定位点之后，由于某种原因被杀死，经过1个小时以后，又被唤醒，继续开始采集59定位点，则此时58和78的ID之差虽然不大于20，但是实际的时间却远超过10min，因此需要比较58和78的时间之差，如果不超过10min，则是正常的情况，58—78之间的Stop点都满足Temporal维度的限制，如果超过10min，则比较59和78的时间之差，直到某一点与78的时间之差不差过10min；对于78—98之间的点，同理。</p><h1 id="聚类结果对比"><a href="#聚类结果对比" class="headerlink" title="聚类结果对比"></a>聚类结果对比</h1><p>经试验可知：<br>双指针遍历链表算法的正确率相对较高，但是并不稳定，偶尔会出现正确率极低的情况；<br>ST-DBSCAN方法的聚类结果比较准确，而且可以通过调节Eps1参数来更加接近真实结果。对于实际活动数目比较少的情况，不易生成噪声（错误数较少），Eps1应尽量取较小值；对于实际活动数目比较多的情况，易产生噪声（错误数较多），Eps1应尽量取较大值。</p><h1 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h1><p>对于双指针遍历链表的聚类算法，适用于采集定位时间间隔较长，对时空复杂度要求较高的计算；对于我们的ST-DBSCAN算法，适用于采集定位时间间隔较短，对时空复杂度容忍度较高的计算。<br>而ST-DBSCAN算法，目前我们的研究只对Eps1的敏感度做了实验，并没有对Eps2和MinPts进行实验。根据经验，可以判断出MinPts对于聚类结果也会有一定的影响，接下来的工作可以围绕着MinPts展开。</p>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聚类算法 </tag>
            
            <tag> ST-DBSCAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>web开发之图片存储</title>
      <link href="/2018/03/31/web%E5%BC%80%E5%8F%91%E4%B9%8B%E5%9B%BE%E7%89%87%E5%AD%98%E5%82%A8/"/>
      <url>/2018/03/31/web%E5%BC%80%E5%8F%91%E4%B9%8B%E5%9B%BE%E7%89%87%E5%AD%98%E5%82%A8/</url>
      <content type="html"><![CDATA[<h2 id="1-项目背景"><a href="#1-项目背景" class="headerlink" title="1.项目背景"></a>1.项目背景</h2><p>应实验室导师的需求，我和几位同学需要从零开发一个Android+Java Web的<strong>C/S</strong>系统。</p><h2 id="2-图片存储何处，如何请求"><a href="#2-图片存储何处，如何请求" class="headerlink" title="2.图片存储何处，如何请求"></a>2.图片存储何处，如何请求</h2><p>我们最初设计数据库时，当碰到图片，为了加快进度，就暂时把图片当做一个<strong>BLOB</strong>的类型。</p><p>不过，当我们完全设计好符合功能需求的数据库后，又不得不重新思考起来，图片的存储与获取问题。<a id="more"></a></p><h3 id="2-1-数据库"><a href="#2-1-数据库" class="headerlink" title="2.1 数据库"></a>2.1 数据库</h3><p>几乎所有的数据可以都存放在数据库中，所以，即是对于像图片这样的大文件，数据库也是可以容纳的。</p><p>以MySQL为例，我们可以使用<strong>BLOB</strong>类型来存储图片。</p><p>假定我们有一张新闻表用来存储新闻，新闻有新闻id、标题title、图片pic、文本text以及日期date，如下表所示：</p><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">字段名称</th><th style="text-align:center">类型</th><th style="text-align:center">宽度(字节)</th><th style="text-align:center">主键</th><th style="text-align:center">外键</th><th style="text-align:center">可空</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">ARTICLEID</td><td style="text-align:center">VARCHAR</td><td style="text-align:center">30</td><td style="text-align:center">Y</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">文章ID</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">TITLE</td><td style="text-align:center">VARCHAR</td><td style="text-align:center">40</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">题目</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">PIC</td><td style="text-align:center">BLOB</td><td style="text-align:center"></td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">图片</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">TEXT</td><td style="text-align:center">LONGTEXT</td><td style="text-align:center"></td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">文本</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">DATE</td><td style="text-align:center">DATETIME</td><td style="text-align:center"></td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">日期</td></tr></tbody></table><p>此时我们的图片作为一个BLOB类型直接存储在数据库中。当客户端希望获取一条新闻，各个系统间的交互如下所示：</p><p><img src="/2018/03/31/web开发之图片存储/1522672614773.png" alt="52267261477"></p><p>可以发现，一条新闻的所有内容都是同步返回给客户端的。也就是说客户端需要在所有数据都接收到后，才能把新闻显示给用户。</p><blockquote><ol><li>网络slowly并且unstable</li><li>用户是没有耐心的</li></ol></blockquote><p>如果一条新闻，用户等了3秒还没有看到的话，他也就不会看了。所以，同步返回内容和图片是不可行的。</p><h3 id="2-2-文件系统"><a href="#2-2-文件系统" class="headerlink" title="2.2 文件系统"></a>2.2 文件系统</h3><p>以上，我们看到把图片存储在数据库是有很大缺陷的。所以我们需要将图片和普通数据分离，把图片存入文件系统中，而把图片的地址存入数据库。</p><p>于是我们把新闻表改为如下设计：</p><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">字段名称</th><th style="text-align:center">类型</th><th style="text-align:center">宽度（字节）</th><th style="text-align:center">主键</th><th style="text-align:center">外键</th><th style="text-align:center">可空</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">ARTICLEID</td><td style="text-align:center">VARCHAR</td><td style="text-align:center">30</td><td style="text-align:center">Y</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">文章ID</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">TITLE</td><td style="text-align:center">VARCHAR</td><td style="text-align:center">40</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">题目</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">PIC</td><td style="text-align:center">TEXT</td><td style="text-align:center"></td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">图片URL</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">TEXT</td><td style="text-align:center">LONGTEXT</td><td style="text-align:center"></td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">文本</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">DATE</td><td style="text-align:center">DATETIME</td><td style="text-align:center"></td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">日期</td></tr></tbody></table><p>那么客户端请求一条新闻的流程就会变得如下所示：</p><p><img src="/2018/03/31/web开发之图片存储/1522672667043.png" alt="52267266704"></p><p>可以发现，应用服务器只返回了少量文本信息给客户端，客户端收到消息并解析之后，可以先把普通的文本内容呈现给用户（减少了用户的等待时间），然后客户端再异步的向图片服务器请求图片，收到图片数据后再显示给用户。<br>细心地小伙伴也会发现，我们访问一些网页经常会出现图片加载不出来的情况，这也侧面说明图片等大文件的传输是多么的不稳定，所以内容和图片分离的必要性也就显而易见了。</p><h2 id="3-如何存储"><a href="#3-如何存储" class="headerlink" title="3. 如何存储"></a>3. 如何存储</h2><p>我们讨论了如何获取数据，那么如何上传数据呢？</p><p>没错，和我们之前讨论过的图片获取过程类似，就是像下图所示了：</p><p><img src="/2018/03/31/web开发之图片存储/1522672700003.png" alt="52267270000"></p>]]></content>
      
      <categories>
          
          <category> Java web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java web </tag>
            
            <tag> 图片 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
